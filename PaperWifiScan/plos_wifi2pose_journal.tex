% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Moving 3D Pose Estimation with ESP32 Wi-Fi} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Name1 Surname\textsuperscript{1,2\Yinyang},
Name2 Surname\textsuperscript{2\Yinyang},
Name3 Surname\textsuperscript{2,3\textcurrency},
Name4 Surname\textsuperscript{2},
Name5 Surname\textsuperscript{2\ddag},
Name6 Surname\textsuperscript{2\ddag},
Name7 Surname\textsuperscript{1,2,3*},
with the Lorem Ipsum Consortium\textsuperscript{\textpilcrow}
\\
\bigskip
\textbf{1} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{2} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\textbf{3} Affiliation Dept/Program/Center, Institution Name, City, State, Country
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
\ddag These authors also contributed equally to this work.

% Current address notes
\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
\dag Deceased

% Group/Consortium Author Note
\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* correspondingauthor@institute.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
Doraemon.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
\section*{Author summary}
Doraemon.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
 Wifi is one of common network mediums nowadays. Normally, It is used for establishing a wireless network to connect to the internet. But there are still many more functions Wifi is good at. Wifi can also be applied in fields beside connecting to the internet according to its stability being upgraded continuously. Decent Wifi connectivity can extract more data other than the data to be transmitted like concentration, speed, obstacle between the transmission. Those can be composed to be many useful application on their own like localization, activity prediction and  etc.


Camera is a very good tool for monitoring things and is being used as a very effective data collector for mapping to the ground truth to create many popular machine-learning-based usable model like pose estimation, text segmentation, object detection and many  more. However, camera may be unavoidably judged as a serious privacy infringement since the data obtained like a photo or video are too clear and possess too much information that might be used in a bad way.


There are many works tried to extract those extractable features like camera's videoes does from Wifi. But, they are mostly working with very specific tools and Network Interface Card (NIC) connected to a labtop running Linux that is currently one of the ways allowing to obtain fine-grain Channel State Information (CSI), the descriptive data of the  Wifi propagating in that environment. Those limitation significantly decrease steamline of implementation. It is hard for public demonstration and intregation with many updated tools in operating system like Windows or OSX.


Actually, there are other existing ways for obtaining the CSI. One is from a ubiquitously used microprcessor, ESP32. which is still not much be explored in Wifi exploiting field. It is simple to implement and can be easily integrated with others tools in many platforms due to its massively produced external tools. 

Human pose estimation is one of the most popular topic in machine-learning-based field. It can be used as visualization directly and also applied for further applications like Game-controlling, Activity classification, Violence detection and many more. Most of the model extract human pose from videos which are taken from camera which conduct privacy issue like statement above. We are expecting to extract human pose from Wifi CSI to solve the issue. So, this paper proposes a machine-learning-based model to create a mapping rule from Wifi CSI to 3D moving human pose estimation by using ESP32.


\section*{Background}

\subsection*{Pose Estimation}

There many machine-learning-based human body pose estimation tools had been proposed and available online. Those can de found both 2D and 3D. In our paper, we chose a light weight 3D pose estimation as an annotation due to its simplicity and the hypothesis that 3D should suit the most in our work. The project can be found at [github-lw3d] and is based on [paper-Lightweight OpenPose] and [paper- Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB]. Its job is to simply create 3D human pose annotation from an image. Then, feed to our works training process as annoations.


\subsection*{Wifi}\label{wifi}

Wifi is a well-known connectivity with no wire needed (wireless). It has been used as a medium for connecting to the internet for over 10 years. However, the Wifi is the name covering IEEE 802.11 n/g/ac protocols. It mostly deliver data through 2.4/5GHz frequency with multiple channels. The bandwidth in each channel is 22MHz. the data are to be transmitted  pararelly with multiplexing technique named orthogonal frequency division multiplexing (OFDM). Each carrier may propagate to a reciever with encountering many obstacles. The effect of that situation is the Doppler Effect.
So, Channel State Information (CSI) is represented as physical layer indicator that can be used to investigate how each channel propagate to the reciever or back to the transmitter.

If a sender sends data to a reciever through Wifi, the data will be surely not transmitted without any loss.



\subsection*{CSI data}\label{CSI}
As mentioned in \ref{wifi} that data propagating to the reciever while touching surrounding environment, the CSI is a variation of the data. The CSI can  be found at both sender and reciever since reciever may transmit data back. In this paper, We consider to mainly use CSI at the transmiter. Let the sender use the modulation method of 16-quadrature amplitude modulation (16-QAM) which one carrier can carry 4 bits. When the sender needs to send a '1111', the modulation returns $x=1+1i$ then, transmit to the reciever. At the reciever, let the obtained data is $y=0.8+0.9i$. So, the CSI can be computed by the variation $h=y/x=0.2+3.4i$.

Human body is literally water which reflect radio wave like Wifi. [] and [] have proven that human body can affect the CSI.

\subsubsection*{ESP32}\label{ESP32}
ESP32 is a very popular single-board computer (SBC). With its affordable price and many available additional tools, ESP32 is commonly used in Internet of Things field. Moreover, it can be applied in research field. Quantitative CSI can be obtained from Wifi in ESP according to [Wi-ESP]. The number of available subcarriers in ESP32 is 64.

According to the detail about Wifi mentioned in \ref{wifi}, the Wifi in ESP32 has some limitation. It supports only 2.4GHz frequency and can be set only one channel over a connection. The bandwidth of each channel is 22MHz. The CSI can be both obtain from Access point (AP) and station (STA) as shown in Fig~\ref{fig:ESP32CSI01}.
The frequency of each channel is as 802.11 standard.



\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=70mm,scale=0.5]{ESP32CSI01.png}}
	\caption{CSI from ESP32s with channel 6.}
	\label{fig:ESP32CSI01}
\end{figure}

\iffalse
\subsubsection*{Faraday cage}
Faraday cage is invented by Michael Faraday in 1836. It is an enclosure to block electromagnetic fields. It is made of conducting material which can affect any radio frequency (e.g. Wifi) to be unable to pass through.
\fi

\section*{Materials and methods}

\subsection*{Concept}
\label{concept}

\begin{figure}[htbp]
	
	\centerline{\includegraphics[width=70mm,scale=0.5]{ESP32CSI02.png}}
	\caption{2 different CSIs resulted from corresponding human poses.}
	\label{fig:ESP32CSI02}
\end{figure}

Other famous proposed works like [] [] and [] focus on line-of-sight (LOS) in between AP and STA while our work uses 2 directional Wifi antennae and focus on reflection from human body as shown in Fig~\ref{fig:ESP32CSI02} on the left.


The reason we name ``Moving Pose Estimation" instead of ``Pose Estimation" is that CSI is not only affected by human body but mainly by overall environment. This means that 2 corresponding human poses can result obviously different CSIs if the environment around are not exactly the same as shown in Fig~\ref{fig:ESP32CSI02}.


So, the detection of human standing still in every environment is nearly impossible since the CSI of that situation may be found exactly matched to a CSI of the environment that a big bottle of water placed in front of ESP32.
In short, if it does not move, we do not if it is human.

Meanwhile, the moving pose is totally different because we focus on its change instead. The example of mapping CSI's change to Activity Classification can be found in [] and []. Our work does likewise but focusing on Pose Estimation instead of Activity Classification.

In different environment, the CSIs are  different. But, the corresponding moving pose may affect to the same changing pattern of CSI. This hypothesis is investigated in the upcoming parts.






All steps of training method is shown in Fig~\ref{fig:TRAINSTEP}. 


\begin{figure}[htbp]
	\centerline{\includegraphics[width=55mm,scale=0.2]{TRAINSTEP06.png}}
	\caption{All steps of training method.}
	\label{fig:TRAINSTEP}
\end{figure}



\subsection*{Pre-processing}\label{Processing}

\subsubsection*{CSI Resampling}


As mentioned in \ref{ESP32}, There are 64 subcarriers in CSI data from ESP32 but there are only 52 those are usable while the rest are null. So, we can construst a tensor of $1 \times 52$ to represent each CSI. We are to map CSI from the ESP32 to 3D human pose annotation from a camera. The sampling rate of the camera are set to 30Hz. So, we have 30 human pose annotations for one second. For the ESP32, the sampling rate is originally unpredictable and not constant but it is running around 120Hz. So we do a process called ``Resampling" to obtain CSI at rate 30Hz in order to map to each human pose annotation.

An example of CSI Resampling is shown in Fig~\ref{fig:CSIResampling01}. The top graph shows that the the original CSI is logged unstably. The bottom one is to pick a timestamp at rate 30Hz and calculated each with the closet data from the original with a simple mathamatical weight equation as in Eq.~\ref{eq:CSIResampling01} in order to predict CSI at timestamp corresponding to each human pose annotation.

\begin{equation}
\begin{aligned}
& CSI_{now} = CSI_{before} \\ 
& + \left(  \frac{ts_{now}-ts_{before}}{ts_{after}-ts_{before}}  \times (CSI_{after}-CSI_{before})   \right)
\label{eq:CSIResampling01}
\end{aligned}
\end{equation},
where ....

\begin{figure}[htbp]
	\centerline{\includegraphics[width=70mm,scale=0.5]{CSIResampling01.png}}
	\caption{An example of CSI resampling.}
	\label{fig:CSIResampling01}
\end{figure}

Now we can map each CSI sample to each 3D human pose annotation with the corresponding timestamp.


\subsubsection*{Pose Estimation and PAM formation}

We use [github-lw3d] to estimate 3D human pose from image as stated above. the estimation gives us a $19 \times 3$ matrix for each image. The $19 \times 3$ matrix are used as an annotation where $19$ is for keypoints in human body and $3$ is for 3 axes coordination as shown in Fig~\ref{fig:POSEkeypoint}. But, the annotation can still possess too much independency. Some alignment of keypoints may lead to some impossible pose e.g. body keypoint found very far from neck keypoint or head keypoint attached to hip keypoint. We assume those poses are quite not possible for normal human pose. To preserve those constraints, we form a pose adjacent matrix (PAM) from an original $19\times3$ matrix. the PAM is applied for all x, y and z axes. Each are to be form their $19\times19$ matrix by the following equations.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=20mm,scale=0.5]{POSEkeypoint01.png}}
	\caption{19 keypoints of human pose.}
	\label{fig:POSEkeypoint}
\end{figure}

\begin{equation}
x_{i,j}' = \begin{cases}
x_i-x_j &\text{$i\neq j$}\\
x_i &\text{$i=j$}\\
\end{cases}
\label{eq:xPAM}
\end{equation},
\begin{equation}
y_{i,j}' = \begin{cases}
y_i-y_j &\text{$i\neq j$}\\
y_i &\text{$i=j$}\\
\end{cases}
\label{eq:yPAM}
\end{equation}
and
\begin{equation}
z_{i,j}' = \begin{cases}
z_i-x_j &\text{$i\neq j$}\\
z_i &\text{$i=j$}\\
\end{cases}
\label{eq:zPAM}
\end{equation}.

The PAM is finally a $3\times19\times19$ matrix created from 3 matrices of $x'$,$y'$ and $z'$ stacked.
Apparently, one PAM represent one human pose.



Conclusively, we are making a model by mapping a sequence of $1\times52$ matrix from CSI to each sequence of $ 19 \times 3 \times 3 $ PAM from moving human pose annotation with the corresponding timestamp as shown in Fig~\ref{fig:STEP01}. 

\begin{figure}[htbp]
	\centerline{\includegraphics[width=90mm,scale=0.5]{STEP05.png}}
	\caption{A mapping rule from CSI frames to 3D human poses.}
	\label{fig:STEP01}
\end{figure}



\subsection*{Processing}


\subsubsection*{Mapping CSI and PAM}
Let $D$ be a set of synchronized pose and CSI data package. Each pair has corresponding timestamp.

\begin{equation}
D =  {(C_t, P_t), t \in [1, n]},
\label{eq:Dataset}
\end{equation} where  $n$ is a number of pairs, $C$ is for CSI data from ESP32, $P$ is for PAM annotation as the ground truth, $t$ is the timestamp index when those 2 were collected.



\subsubsection*{Grouping By Sequence Size}

Let $\sigma$ be an adjustable window size. $\sigma$ size of $D$ are fed to the model solely. So,  $\sigma \le n$. And, Let $m$ be the number of feeding iteration, $m = \lfloor \frac{n}{\sigma} \rfloor$.


\subsubsection*{Forming Network Layer}

The network swallow $m$ training data as an input, where each is a sequencial set of $(P_t,C_t)$ at a corresponding timestamp with size $\sigma$.
Let $\Gamma$ be a representation of each sequencial set.

\begin{equation}
\Gamma =  {(C_u, P_u), u \in [1, \sigma]},
\label{eq:SubDataset}
\end{equation}
where $u$ is the timestamp index when those 2 were collected. Apparently, the number of $\Gamma$ is $m$.

As the $\Gamma$ is a sequencial set with size $\sigma$, Long-short term memory (LSTM) [] is suitable for this type of data.

\paragraph*{(Optional) CSI Feature Extractor}
We may extract feature from the CSI before feed it to the model. Or ignore it and feed original CSI to the model directly.

\paragraph*{(Optional) CSI Feature Engineering}
As mentioned in \nameref{concept}, CSI ($C$) value implies environment where signal propagating and we need to ignore it in order to universalize the circumstance. It means variousity on CSI data can confuse the model significantly. To solve it, we simply sequentially substract $C_u$  in each $\Gamma$ backward to preserve only how CSI changes over each sequence by the following equation,


\begin{equation}
C_u =  \begin{cases}
\text{all } 0 &\text{$u = 1$}\\
C_u - C_{u-1} &\text{$u > 1 $}\\
\end{cases}, C_u \in \Gamma.
\label{eq:CSIFeatureEn}
\end{equation}



\paragraph*{CSI Sequence to Pose Sequence (CSPS)}

The summation of layers is shown in Table~\ref{table:CSPS}.
It is designed to shape a CSI Sequence ( $\sigma \times 52$ tensor) to a predicted Pose Sequence ( $\sigma \times 3\times 19\times 19$ tensor).

\begin{table}[!ht]
	\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
		\centering
		\caption{
		{\bf Layers in CSPS.}}
		\begin{tabular}{lll}
			\hline
			Layer (type)       & Output Shape     & Param \# \\ 
			\thickhline
			Bidirectional LSTM & (None, 200)      & 122400   \\
			RepeatVector       & (None, $\sigma$, 200)  & 0        \\
			Bidirectional LSTM & (None, $\sigma$, 200)  & 240800   \\
			TimeDistributed over Dense    & (None, $\sigma$, 1083) & 217683   \\ \hline
		\end{tabular}
	
		\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
		\end{flushleft}
		\label{table:CSPS}
	\end{adjustwidth}
\end{table}


We first use Bidirectional LSTM as an encoder layer with the input size of $(\sigma,52)$ to satisfy dimension of sequential $C_u$ in $\Gamma$.
Then, the repeat vector is added with time $\sigma$ to make the model treat with correct time-step. Afterward, we placed a decoder layer with another Bidirectional LSTM. 

Next, we dense the decoder to be $1083$ where can be reshaped into $3\times 19\times 19$ later.

Lastly,  the TimeDistributed layer is used in order to make the model treat the output for each time-step individually.



% Results and Discussion can be combined.
\section*{Results}

\subsection*{Data Collection}
dance in ma room
((
We collected data under an approval of Carnegie Mellon University IRB 4
. We recruited 8 volunteers,
and asked them to do casual daily actions in two rooms of the campus, one laboratory room and one
class room. Floor plans and data collection positions are illustrated in Fig. 8. During the actions, we
run the system in Fig. 3 to record CSI samples and videos, simultaneously. For each volunteer, data
of his first 80% recording is used to train the networks, and data of the last 20% recording is used to
test the networks. The data size of training and testing are 79496 and 19931, respectively))


\subsection*{Experimental Result}


All steps of testing method is shown in Fig~\ref{fig:TESTSTEP}. 

\begin{figure}[htbp]
	\centerline{\includegraphics[width=40mm,scale=0.2]{TESTSTEP06.png}}
	\caption{All steps of testing method.}
	\label{fig:TESTSTEP}
\end{figure}


Percentage of Correct Keypoint (PCK) is widely used to evaluate the performance of human pose estimation according to [].

\begin{equation}
\begin{aligned}
PCK_i@a = \frac{1}{N} \sum_{i=1}^{N}
I(
\frac{\mid \mid  pd_i \text{ - } gt_i \mid \mid^2_2}{\sqrt{rh^2+lh^2}}  \le a ),
\label{eq:PCK}
\end{aligned}
\end{equation}


where $I$(Â·) is a binary indicator that outputs 1 while true and 0 while false, 
$N$ is the number of frames, $i$ is the index of keypoints that $i \in {1, 2, ..., 19}$. The $rh$ and
$lh$ are for the positions of the right shoulder and the left hip from the ground truth, respectively.
So, the ${\sqrt{rh^2+lh^2}}$ is  considered as the length of the upper limb from the ground truth, which is used to normalize the prediction error length
$ \mid \mid pd_i \text{ - } gt_i \mid \mid _2^2$, and $pd_i$ and $gt_i$ are coordinates of prediction and ground-truth at the keypoint $i$ repectively.


Table~\ref{table:PCK} shows the estimation performance of 19 body keypoint in PCK@5, PCK@10, PCK@20,
PCK@30, PCK@40, and PCK@50 of the $\sigma$ $15$.




Github$\footnote[1]{https://github.com/rtmtree/}$.



% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Table of PCK.}}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
	\hline
	Order & Keypoint & PCK@5 & PCK@10 & PCK@20 & PCK@30 & PCK@40 & PCK@50 \\ \thickhline
	1     &          &       &        &        &        &        &        \\ \hline
	2     &          &       &        &        &        &        &        \\ \hline
	3     &          &       &        &        &        &        &        \\ \hline
	4     &          &       &        &        &        &        &        \\ \hline
	5     &          &       &        &        &        &        &        \\ \hline
	6     &          &       &        &        &        &        &        \\ \hline
	7     &          &       &        &        &        &        &        \\ \hline
	8     &          &       &        &        &        &        &        \\ \hline
	9     &          &       &        &        &        &        &        \\ \hline
	10    &          &       &        &        &        &        &        \\ \hline
	11    &          &       &        &        &        &        &        \\ \hline
	12    &          &       &        &        &        &        &        \\ \hline
	13    &          &       &        &        &        &        &        \\ \hline
	14    &          &       &        &        &        &        &        \\ \hline
	15    &          &       &        &        &        &        &        \\ \hline
	16    &          &       &        &        &        &        &        \\ \hline
	17    &          &       &        &        &        &        &        \\ \hline
	18    &          &       &        &        &        &        &        \\ \hline
	19    &          &       &        &        &        &        &        \\ \hline
\end{tabular}
\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
\end{flushleft}
\label{table:PCK}
\end{adjustwidth}
\end{table}




\section*{Discussion}
Doraemon.
% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
	\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
		\centering
		\caption{
			{\bf Table caption Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam.}}
		\begin{tabular}{|l+l|l|l|l|l|l|l|}
			\hline
			\multicolumn{4}{|l|}{\bf Heading1} & \multicolumn{4}{|l|}{\bf Heading2}\\ \thickhline
			$cell1 row1$ & cell2 row 1 & cell3 row 1 & cell4 row 1 & cell5 row 1 & cell6 row 1 & cell7 row 1 & cell8 row 1\\ \hline
			$cell1 row2$ & cell2 row 2 & cell3 row 2 & cell4 row 2 & cell5 row 2 & cell6 row 2 & cell7 row 2 & cell8 row 2\\ \hline
			$cell1 row3$ & cell2 row 3 & cell3 row 3 & cell4 row 3 & cell5 row 3 & cell6 row 3 & cell7 row 3 & cell8 row 3\\ \hline
		\end{tabular}
		\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
		\end{flushleft}
		\label{table1}
	\end{adjustwidth}
\end{table}

\section*{Conclusion}

Doraemon.

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Fig.}
\label{S1_Fig}
{\bf Bold the title sentence.} Add descriptive text after the title of the item (optional).

\paragraph*{S2 Fig.}
\label{S2_Fig}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 File.}
\label{S1_File}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Video.}
\label{S1_Video}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Appendix.}
\label{S1_Appendix}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Table.}
\label{S1_Table}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\section*{Acknowledgments}
	We thank Sirindhorn International Institute of Technology for providing technical environment and supportive information.



\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}

\bibitem{bib1}
Conant GC, Wolfe KH.
\newblock {{T}urning a hobby into a job: how duplicated genes find new
  functions}.
\newblock Nat Rev Genet. 2008 Dec;9(12):938--950.

\bibitem{bib2}
Ohno S.
\newblock Evolution by gene duplication.
\newblock London: George Alien \& Unwin Ltd. Berlin, Heidelberg and New York:
  Springer-Verlag.; 1970.

\bibitem{bib3}
Magwire MM, Bayer F, Webster CL, Cao C, Jiggins FM.
\newblock {{S}uccessive increases in the resistance of {D}rosophila to viral
  infection through a transposon insertion followed by a {D}uplication}.
\newblock PLoS Genet. 2011 Oct;7(10):e1002337.

\end{thebibliography}



\end{document}

